# âœˆï¸ AWS Glue ETL Airline Data Pipeline

> **Event-driven serverless data pipeline for flight delay analytics with automated orchestration and monitoring**

[![AWS Glue](https://img.shields.io/badge/AWS%20Glue-ETL-FF9900?style=flat-square&logo=amazon-aws)](https://aws.amazon.com/glue/)
[![Step Functions](https://img.shields.io/badge/Step%20Functions-Orchestration-FF9900?style=flat-square&logo=amazon-aws)](https://aws.amazon.com/step-functions/)
[![Redshift](https://img.shields.io/badge/Redshift-Data%20Warehouse-FF9900?style=flat-square&logo=amazon-aws)](https://aws.amazon.com/redshift/)
[![EventBridge](https://img.shields.io/badge/EventBridge-Event%20Bus-FF9900?style=flat-square&logo=amazon-aws)](https://aws.amazon.com/eventbridge/)

## ğŸ¯ Overview

An **event-driven ETL pipeline** that automatically processes daily airline flight data, enriches it with airport dimensions, and loads delay analytics into Redshift. Built with AWS serverless services featuring crawler orchestration, PySpark transformations, and SNS-based monitoring.

### âœ¨ Key Features

- **âš¡ Event-Driven Architecture** - S3 uploads trigger automated pipeline execution
- **ğŸ”„ Step Functions Orchestration** - Multi-step workflow with error handling
- **ğŸ¤– Glue Crawlers** - Auto-catalog schema discovery (3 crawlers)
- **ğŸ”§ PySpark ETL** - Complex joins and transformations for delay analysis
- **ğŸ’¾ Redshift Analytics** - Denormalized fact table for BI queries
- **ğŸ“§ SNS Notifications** - Success/failure alerts for pipeline monitoring
- **ğŸ—‚ï¸ Hive Partitioning** - Daily partitioned data for efficient processing

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA SOURCE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ Daily Flight Data (CSV)                                 â”‚
â”‚  â€¢ Hive-style partitions: year=YYYY/month=MM/day=DD/        â”‚
â”‚  â€¢ Fields: carrier, origin/dest airports, delays            â”‚
â”‚  ğŸ“ Airport Dimension (CSV)                                  â”‚
â”‚  â€¢ Static reference: airport_id, city, state, name          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INGESTION LAYER                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“¦ Amazon S3 Bucket                                        â”‚
â”‚  â€¢ Folder: dim/airports.csv (dimension)                     â”‚
â”‚  â€¢ Folder: daily_flights/ (partitioned fact data)           â”‚
â”‚  â€¢ EventBridge notifications enabled                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                   ğŸ”” S3 Object Created Event
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   EVENT DETECTION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“¡ Amazon EventBridge Rule                                 â”‚
â”‚  â€¢ Event Pattern: Object Created (*.csv suffix)             â”‚
â”‚  â€¢ Target: Step Function state machine                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ORCHESTRATION LAYER                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”„ AWS Step Functions State Machine                        â”‚
â”‚                                                              â”‚
â”‚  1. StartCrawler                                             â”‚
â”‚     â””â”€â†’ Trigger: flights-data-crawler                       â”‚
â”‚                                                              â”‚
â”‚  2. GetCrawler (polling loop)                                â”‚
â”‚     â””â”€â†’ Check crawler state                                 â”‚
â”‚                                                              â”‚
â”‚  3. Is_Running? (Choice state)                               â”‚
â”‚     â”œâ”€â†’ RUNNING â†’ Wait 5 seconds â†’ GetCrawler               â”‚
â”‚     â””â”€â†’ READY â†’ Glue StartJobRun                            â”‚
â”‚                                                              â”‚
â”‚  4. Glue StartJobRun (.sync)                                 â”‚
â”‚     â”œâ”€â†’ Job: airline_data_ingestion                         â”‚
â”‚     â”œâ”€â†’ Success â†’ Glue_Job_Status                           â”‚
â”‚     â””â”€â†’ Failure â†’ Failed_Notification (SNS)                 â”‚
â”‚                                                              â”‚
â”‚  5. Glue_Job_Status (Choice state)                           â”‚
â”‚     â”œâ”€â†’ SUCCEEDED â†’ Success_Notification                    â”‚
â”‚     â””â”€â†’ FAILED â†’ Failed_Notification                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Step Function Workflow

![Step Function Workflow] <img width="569" height="470" alt="image" src="https://github.com/user-attachments/assets/555b8187-f050-4e4c-b0f9-1c628978cc5c" />

*Automated orchestration with crawler polling, job execution, and SNS notifications*

---

## ğŸ”§ Glue ETL Job Architecture

![Glue ETL Job] <img width="567" height="647" alt="image" src="https://github.com/user-attachments/assets/ae5bbf98-d014-4efa-a2ac-7b32e6563b27" />

*PySpark transformation flow: Filter â†’ Join â†’ Enrich â†’ Load to Redshift*

---

## ğŸ”„ ETL Process Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               DATA CATALOGING                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¤– AWS Glue Crawlers (3 total)                             â”‚
â”‚                                                              â”‚
â”‚  1. flights-data-crawler (S3)                                â”‚
â”‚     â””â”€â†’ Catalogs daily_flights/ partitions                  â”‚
â”‚                                                              â”‚
â”‚  2. airline_dim_crawler (Redshift)                           â”‚
â”‚     â””â”€â†’ Catalogs airports_dim table                         â”‚
â”‚                                                              â”‚
â”‚  3. airline_fact_crawler (Redshift)                          â”‚
â”‚     â””â”€â†’ Catalogs daily_flights_fact table                   â”‚
â”‚                                                              â”‚
â”‚  ğŸ“š Glue Data Catalog: airlines-table-catalog                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               TRANSFORMATION LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš¡ AWS Glue ETL Job (PySpark)                              â”‚
â”‚                                                              â”‚
â”‚  Step 1: Extract                                             â”‚
â”‚  â€¢ Read daily_flights from Glue Catalog                     â”‚
â”‚  â€¢ Read airports_dim from Redshift (via JDBC)               â”‚
â”‚                                                              â”‚
â”‚  Step 2: Filter                                              â”‚
â”‚  â€¢ Identify delayed flights (depdelay > 60 min)             â”‚
â”‚                                                              â”‚
â”‚  Step 3: Enrich - Departure Join                             â”‚
â”‚  â€¢ Join flights with airports_dim on originairportid        â”‚
â”‚  â€¢ Extract: dep_city, dep_airport, dep_state                â”‚
â”‚                                                              â”‚
â”‚  Step 4: Enrich - Arrival Join                               â”‚
â”‚  â€¢ Join with airports_dim on destairportid                  â”‚
â”‚  â€¢ Extract: arr_city, arr_airport, arr_state                â”‚
â”‚                                                              â”‚
â”‚  Step 5: Transform Schema                                    â”‚
â”‚  â€¢ ApplyMapping for Redshift compatibility                  â”‚
â”‚  â€¢ Cast data types (VARCHAR, BIGINT)                        â”‚
â”‚                                                              â”‚
â”‚  Step 6: Load                                                â”‚
â”‚  â€¢ Write to Redshift: airlines.daily_flights_fact           â”‚
â”‚  â€¢ Use S3 temp directory for staging                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               DATA WAREHOUSE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ—„ï¸ Amazon Redshift Cluster                                â”‚
â”‚                                                              â”‚
â”‚  Schema: airlines                                            â”‚
â”‚                                                              â”‚
â”‚  â€¢ airports_dim (dimension table)                            â”‚
â”‚    - airport_id, city, state, name                          â”‚
â”‚    - 300+ U.S. airports                                      â”‚
â”‚                                                              â”‚
â”‚  â€¢ daily_flights_fact (fact table - denormalized)           â”‚
â”‚    - carrier, dep_delay, arr_delay                          â”‚
â”‚    - dep_city, dep_airport, dep_state                       â”‚
â”‚    - arr_city, arr_airport, arr_state                       â”‚
â”‚    - Pre-joined for fast BI queries                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               MONITORING & NOTIFICATIONS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“§ Amazon SNS Topic                                        â”‚
â”‚  â€¢ Success notifications: Job completed                     â”‚
â”‚  â€¢ Failure notifications: Error details + logs              â”‚
â”‚  â€¢ Email/SMS alerts to data team                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Technical Stack

### **Core AWS Services**
- **âš¡ AWS Glue** - Serverless ETL with PySpark
- **ğŸ”„ AWS Step Functions** - Workflow orchestration with state machine
- **ğŸ“¡ Amazon EventBridge** - Event-driven pipeline triggers
- **ğŸ—„ï¸ Amazon Redshift** - Petabyte-scale data warehouse
- **ğŸ“¦ Amazon S3** - Data lake storage with partitioning
- **ğŸ“§ Amazon SNS** - Real-time notifications

### **Data Processing**
- **ğŸ PySpark** - Distributed data transformations
- **ğŸ¤– Glue Crawlers** - Auto-schema discovery
- **ğŸ“š Glue Data Catalog** - Centralized metadata repository
- **ğŸ”— JDBC Connection** - Glue â†” Redshift integration

### **Infrastructure**
- **ğŸ” IAM Roles** - Secure service permissions
- **ğŸŒ VPC Configuration** - Network isolation with S3 endpoint
- **ğŸ”’ Security Groups** - Redshift port access control

## ğŸ“Š Data Model

### **Dimension Table: airports_dim**
```sql
CREATE TABLE airlines.airports_dim (
    airport_id BIGINT,
    city VARCHAR(100),
    state VARCHAR(100),
    name VARCHAR(200)
);
-- 300+ U.S. airports with metadata
```

### **Fact Table: daily_flights_fact (Denormalized)**
```sql
CREATE TABLE airlines.daily_flights_fact (
    carrier VARCHAR(10),
    dep_airport VARCHAR(200),
    arr_airport VARCHAR(200),
    dep_city VARCHAR(100),
    arr_city VARCHAR(100),
    dep_state VARCHAR(100),
    arr_state VARCHAR(100),
    dep_delay BIGINT,
    arr_delay BIGINT
);
```

**Design Note:** The fact table is **denormalized** (pre-joined with dimension) to eliminate repeated joins during BI queries, improving query performance by 10-50x.

## ğŸš€ Quick Start

### Prerequisites
- AWS account with appropriate permissions
- AWS CLI configured
- S3 bucket with EventBridge notifications enabled
- Redshift cluster with VPC and S3 endpoint configured

### Setup Instructions

**1. Create S3 Bucket & Upload Data**
```bash
# Create bucket
aws s3 mb s3://airline-data-landing-zn

# Upload dimension data
aws s3 cp airports.csv s3://airline-data-landing-zn/dim/

# Upload daily flight data (partitioned)
aws s3 cp daily_flights/ s3://airline-data-landing-zn/daily_flights/ --recursive

# Enable EventBridge notifications
aws s3api put-bucket-notification-configuration \
  --bucket airline-data-landing-zn \
  --notification-configuration file://s3-notification.json
```

**2. Create Redshift Cluster & Tables**
```sql
-- Create schema
CREATE SCHEMA airlines;

-- Create dimension table
CREATE TABLE airlines.airports_dim (
    airport_id BIGINT,
    city VARCHAR(100),
    state VARCHAR(100),
    name VARCHAR(200)
);

-- Load dimension data
COPY airlines.airports_dim
FROM 's3://airline-data-landing-zn/dim/airports.csv' 
IAM_ROLE 'arn:aws:iam::ACCOUNT:role/redshift-s3-role'
DELIMITER ',' IGNOREHEADER 1 REGION 'us-east-1';

-- Create fact table (populated by Glue)
CREATE TABLE airlines.daily_flights_fact (
    carrier VARCHAR(10),
    dep_airport VARCHAR(200),
    arr_airport VARCHAR(200),
    dep_city VARCHAR(100),
    arr_city VARCHAR(100),
    dep_state VARCHAR(100),
    arr_state VARCHAR(100),
    dep_delay BIGINT,
    arr_delay BIGINT
);
```

**3. Configure VPC & Security**
```bash
# Create S3 VPC endpoint (for Glue-Redshift communication)
aws ec2 create-vpc-endpoint \
  --vpc-id vpc-xxxxx \
  --service-name com.amazonaws.us-east-1.s3 \
  --route-table-ids rtb-xxxxx

# Update Redshift security group (open port 5439)
aws ec2 authorize-security-group-ingress \
  --group-id sg-xxxxx \
  --protocol tcp \
  --port 5439 \
  --cidr 10.0.0.0/16
```

**4. Create Glue Connection (JDBC to Redshift)**
```bash
aws glue create-connection \
  --connection-input '{
    "Name": "redshift-connection",
    "ConnectionType": "JDBC",
    "ConnectionProperties": {
      "JDBC_CONNECTION_URL": "jdbc:redshift://cluster-endpoint:5439/dev",
      "USERNAME": "admin",
      "PASSWORD": "your-password"
    },
    "PhysicalConnectionRequirements": {
      "SubnetId": "subnet-xxxxx",
      "SecurityGroupIdList": ["sg-xxxxx"],
      "AvailabilityZone": "us-east-1a"
    }
  }'
```

**5. Create Glue Crawlers**
```bash
# Crawler 1: S3 daily flights data
aws glue create-crawler \
  --name flights-data-crawler \
  --role GlueServiceRole \
  --database airlines-table-catalog \
  --targets '{"S3Targets": [{"Path": "s3://airline-data-landing-zn/daily_flights/"}]}'

# Crawler 2: Redshift dimension table
aws glue create-crawler \
  --name airline_dim_crawler \
  --role GlueServiceRole \
  --database airlines-table-catalog \
  --targets '{"JdbcTargets": [{"ConnectionName": "redshift-connection", "Path": "dev/airlines/airports_dim"}]}'

# Crawler 3: Redshift fact table
aws glue create-crawler \
  --name airline_fact_crawler \
  --role GlueServiceRole \
  --database airlines-table-catalog \
  --targets '{"JdbcTargets": [{"ConnectionName": "redshift-connection", "Path": "dev/airlines/daily_flights_fact"}]}'
```

**6. Create Glue ETL Job**
```bash
# Upload PySpark script to S3
aws s3 cp aws_glue_etl_job.py s3://aws-glue-assets/scripts/

# Create Glue job
aws glue create-job \
  --name airline_data_ingestion \
  --role GlueServiceRole \
  --command '{
    "Name": "glueetl",
    "ScriptLocation": "s3://aws-glue-assets/scripts/aws_glue_etl_job.py",
    "PythonVersion": "3"
  }' \
  --default-arguments '{
    "--job-bookmark-option": "job-bookmark-enable",
    "--TempDir": "s3://aws-glue-assets/temporary/"
  }'
```

**7. Create SNS Topic**
```bash
aws sns create-topic --name airline-etl-notifications
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:ACCOUNT:airline-etl-notifications \
  --protocol email \
  --notification-endpoint your-email@example.com
```

**8. Create Step Function**
```bash
# Create state machine from JSON definition
aws stepfunctions create-state-machine \
  --name airline-etl-orchestrator \
  --definition file://step_function_code.json \
  --role-arn arn:aws:iam::ACCOUNT:role/StepFunctionsExecutionRole
```

**9. Create EventBridge Rule**
```bash
aws events put-rule \
  --name airline-data-ingestion-trigger \
  --event-pattern file://event_bridge_rule.json

aws events put-targets \
  --rule airline-data-ingestion-trigger \
  --targets "Id"="1","Arn"="arn:aws:states:us-east-1:ACCOUNT:stateMachine:airline-etl-orchestrator"
```

## ğŸ“ Project Structure

```
aws-glue-etl-airline-data-pipeline/
â”œâ”€â”€ aws_glue_etl_job.py              # PySpark ETL transformation script
â”œâ”€â”€ airports.csv                     # Airport dimension data (300+ records)
â”œâ”€â”€ event_bridge_rule.json           # S3 event pattern for pipeline trigger
â”œâ”€â”€ step_function_code.json          # State machine definition (workflow)
â”œâ”€â”€ redshift_table_commands.txt      # DDL & COPY commands
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ step-function-diagram.png    # Orchestration workflow visualization
â”‚   â””â”€â”€ glue-etl-job-diagram.png     # ETL data flow diagram
â””â”€â”€ README.md
```

## ğŸ”§ PySpark ETL Logic

### **Key Transformation Steps:**

**1. Filter Delayed Flights (> 60 minutes)**
```python
Filter.apply(
    frame=DailyFlightsData,
    f=lambda row: (row["depdelay"] > 60),
    transformation_ctx="Filter_node"
)
```

**2. Join with Departure Airport Dimension**
```python
Filter_DF.join(
    AirportDim_DF,
    Filter_DF["originairportid"] == AirportDim_DF["airport_id"],
    "left"
)
```

**3. Rename Departure Columns**
```python
ApplyMapping.apply(
    mappings=[
        ("city", "string", "dep_city", "string"),
        ("name", "string", "dep_airport", "string"),
        ("state", "string", "dep_state", "string")
    ]
)
```

**4. Join with Arrival Airport Dimension**
```python
DepartureEnriched_DF.join(
    AirportDim_DF,
    DepartureEnriched_DF["destairportid"] == AirportDim_DF["airport_id"],
    "left"
)
```

**5. Final Schema Transformation**
```python
ApplyMapping.apply(
    mappings=[
        ("carrier", "string", "carrier", "varchar"),
        ("depdelay", "long", "dep_delay", "bigint"),
        ("arrdelay", "long", "arr_delay", "bigint"),
        # ... departure and arrival airport details
    ]
)
```

**6. Load to Redshift**
```python
glueContext.write_dynamic_frame.from_options(
    frame=FinalData,
    connection_type="redshift",
    connection_options={
        "dbtable": "airlines.daily_flights_fact",
        "connectionName": "redshift-connection",
        "redshiftTmpDir": "s3://aws-glue-assets/temporary/"
    }
)
```

## ğŸŒŸ Key Technical Highlights

### **Event-Driven Architecture**
- âœ… **Fully automated** - No manual intervention required
- âœ… **Real-time triggers** - EventBridge captures S3 uploads instantly
- âœ… **Scalable** - Handles variable daily data volumes

### **Orchestration Excellence**
- âœ… **State management** - Step Functions with choice states & loops
- âœ… **Error handling** - SNS notifications on failures
- âœ… **Sync execution** - `.sync` pattern for Glue job completion

### **ETL Optimization**
- âœ… **Denormalized model** - Pre-joined fact table eliminates runtime joins
- âœ… **Partition pruning** - Hive-style partitions for efficient reads
- âœ… **PySpark transformations** - Distributed processing at scale

### **Monitoring & Observability**
- âœ… **SNS alerts** - Success/failure notifications
- âœ… **CloudWatch logs** - Glue job execution logs
- âœ… **Step Functions history** - Complete audit trail

## ğŸ¯ Real-world Use Cases

- **ğŸ“Š Operations Analytics** - Identify delay patterns for route optimization
- **ğŸ’¼ Business Intelligence** - Carrier performance dashboards
- **ğŸ“ˆ Predictive Maintenance** - Correlate delays with weather/airport conditions
- **ğŸ¢ Executive Reporting** - KPIs on on-time performance
- **ğŸ” Root Cause Analysis** - Investigate chronic delay routes

## ğŸš€ Future Enhancements

- [ ] **ğŸ“Š QuickSight Dashboards** - Real-time delay visualization
- [ ] **ğŸ¤– ML Integration** - Delay prediction models with SageMaker
- [ ] **âš¡ Glue Streaming** - Real-time processing with Kinesis
- [ ] **ğŸ—‚ï¸ Data Partitioning** - Optimize Redshift with SORTKEY/DISTKEY
- [ ] **ğŸ“§ Advanced Alerting** - Anomaly detection with CloudWatch
- [ ] **ğŸ”„ Incremental Processing** - Job bookmarks for delta loads

## ğŸ¤ Contributing

Contributions welcome! Focus areas:
- **ğŸ“Š Additional Analytics** - New delay analysis queries
- **ğŸ”§ Performance Tuning** - Redshift optimization strategies
- **ğŸ§ª Testing** - Unit tests for PySpark transformations
- **ğŸ“ˆ Visualization** - QuickSight dashboard templates

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

**Automated airline analytics with serverless AWS architecture** âœˆï¸â˜ï¸
